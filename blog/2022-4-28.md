---
slug: AKS
title: Production Down
authors: [conrad]
tags: [Azure, Kubernetes, Graph, Identity]
hide_table_of_contents: false
---

# In short Duke, a shit storm.

Thursday morning started just great. We had a deployment into a production Kubernetes cluster failing to pull container images from our Azure Container Registry. The `Azure Service Principal` account secret used by the `Kubernetes` cluster had expired and needed to be renewed.

We got around to addressing the new deployment later in the afternoon when many of our staff had left for the day. One might ask why we managed to tackle this problem in the afternoon if it's production? This is because it had no impact on current solutions on the `Kubernetes` and only prevented new feature deployments that were not urgent. So we created a new service principal using the Azure CLI and updated the Kubernetes cluster credentials. Of course, we had to give permissions to the Container Registry to pull images `acr pull`, and we also drained and deallocated nodes in hopes the new credential would allow permission to pull images. This was a mistake, a bad mistake. As we started draining nodes and deallocating and bringing them back online because they would not reboot... we learned we were in the shit.

Our applications started experiencing a problem with the core DNS service inside Kubernetes and were routing outside our cluster. We issued a priority incident, eventually after chasing logs, firewalls, and all the fun stuff. We made a call to reboot the Kubernetes Service. Fantastic! After the reboot, all our daemon sets and pods were up, and so was our primary graph service. We decided we were ready to call it for the night, but we were interrupted by a keen team member that they were seeing something extremely odd. Low and behold, the pods came up, but many of the replicas started crashing. 

So we had intermittent service of our primary graph service, and we made a call to downgrade the priority. Calls were bouncing from our applications, but at least some were going through. I ended the night in a depressed state as I was sure we could resolve the problem if we continued to stare at the screen and solution. One of our product owners pulled the plug and said, " Let us get back at this tomorrow morning after getting some sleep." Sure enough, we did.

After catching about 4 hours of sleep, because my brain was racking on what could be causing this problem after we all retired for the night, I woke up to take my kids to school. Our team was all present in a call at 8:00 AM on the dot. We learned we had missed a step within the hour while creating the `Service Principal`. It lacked the `network contributor` role to update the Kubernetes Route Tables, which was the root cause of Kubernetes pods were trying to route outside the cluster. After adding `network contributor` all our pods came back to life, and service was restored.

## Conclusion

After describing our mistakes one could reference [Azure - Kubernetes Service Principals ](https://docs.microsoft.com/en-us/azure/aks/kubernetes-service-principal?tabs=azure-cli)

We things went south for us is when we missed the scope in assignment

```bash
az role assignment create --assignee <appId> --scope <resourceScope> --role Contributor
```

:::tip
/subscriptions/<guid>/resourceGroups/myResourceGroup 
/subscriptions/<guid>/resourceGroups/myResourceGroupVnet/providers/Microsoft.Network/virtualNetworks/myVnet
:::

The official wording says or but it was an and in our environment. Simple, but caused a hell of a headache and crunch session in the moment to try and bring the production services back to a fully restored state. 

We attempted to resolve this on our own but several of the credentials and individual rbac prevent a lot of engineers from fixing this due to the ownership of the Service Principal / Subscription roles preventing individuals from adding roles to identity.  We were much relieved when services were restored hope this read might help someone who might encounter this.